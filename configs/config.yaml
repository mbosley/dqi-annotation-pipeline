# Global settings
global:
  test: false # If true, run with a small test dataset
  test_size: 3
  dry_run: false # If true, simulate API calls without actually making them
  log_level: "DEBUG" # Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  random_seed: 1990

# Data settings
data:
  input_file: "data/raw/USfinal-clean.csv"
  merged_data_file: "data/clean/merged_us_data_subset.csv"
  validation_data: "data/clean/us_label_data_clean.csv"
  output_directory: "data/prompts"
  clean_data_directory: "data/clean"
  train_data_file: "data/clean/train_data.csv"
  test_data_file: "data/clean/test_data.csv"
  merged_validation_results_train: "data/prompts/master_validation_results_train.csv"
  merged_validation_results_test: "data/prompts/master_validation_results_test.csv"
  example_set_directory: "data/icl_examples"
  test_size: 0.2  # Proportion of data to use for testing

# ICL settings
icl:
  use_in_depth_justifications: false  # Set to true for detailed justifications, false for minimalist approach
  justification_model:
    provider: "openai"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    model_params:
      model: "gpt-4"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: "Here is your justification:\n"
      prepend_str: ""

# JSON schema and healing settings
json_schema:
  path: "schemas/dqi_schema_simple.json"  # Path to your JSON schema file
  healing:
    max_attempts: 3  # Maximum number of healing attempts per response
    healer_model:
      provider: "deepseek"
      api_key: "${DEEPSEEK_API_KEY}"
      base_url: "https://api.deepseek.com"
      model_params:
        model: "deepseek-coder"
        max_tokens: 4000
        temperature: 0.7
        top_p: 1
        preload_str: "Here is your requested JSON:\n{"
        prepend_str: "{"

analysis:
  output_directory: "results/analysis"
  confidence_level: 0.95
  significance_level: 0.05
  metrics_to_analyze:
    - mse
    - correlation
    - accuracy
    - f1
    - oci
  aggregate_metrics:
    - avg_mse
    - avg_correlation
    - avg_accuracy
    - avg_f1
    - avg_oci
    - micro_f1
    - macro_f1
  tests:
    perform_t_tests: true
    perform_tukey_hsd: true
  export_confusion_matrices: true  # New flag to control confusion matrix export
  simplify_dimensions: true
  visualizations:
    generate_heatmaps: true
    generate_bar_plots: true
    generate_radar_charts: true
    generate_cost_comparison: true
    generate_performance_vs_cost: true
    generate_dimension_heatmap: true
    generate_significance_heatmap: true
  export:
    json: true
  dimension_level_analysis: true

# - `k`: This parameter determines how many previous speeches to include in the prompt. It specifies the number of previous speeches to look back from the current speech number.
# - `n`: This parameter limits the total length of the concatenated previous speeches text. It ensures that the final text does not exceed a certain length.
# - `m`: This parameter limits the length of each individual previous speech text. It ensures that each previous speech included in the final text does not exceed a certain length.
# - `p`: This parameter limits the length of the current speech text. It ensures that the text of the current speech does not exceed a certain length.
# Model configurations
models:
  - name: "deepseek-coder-icl-0"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 0
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-icl-5"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 5
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-icl-50"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 50
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-icl-100"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 100
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  # - name: "deepseek-coder-short"
  #   provider: "deepseek"
  #   api_key: "${DEEPSEEK_API_KEY}"
  #   base_url: "https://api.deepseek.com"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 3000
  #     include_previous_speeches: False
  #     n_icl_examples: 0
  #     elements:
  #       - "prompts/json/opening-instructions.txt"
  #       - "prompts/json/dqi-categories.txt"
  #       - "prompts/json/json-schema.txt"
  #       - "prompts/json/example1-json.txt"
  #       - "prompts/json/closing-instructions.txt"
  #   model_params:
  #     model: "deepseek-coder"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     preload_str: "Here is your requested JSON:\n{"
  #     prepend_str: "{"

  # - name: "deepseek-coder-long"
  #   provider: "deepseek"
  #   api_key: "${DEEPSEEK_API_KEY}"
  #   base_url: "https://api.deepseek.com"
  #   prompt:
  #     elements:
  #       - "prompts/json/opening-instructions.txt"
  #       - "prompts/json/dqi-categories.txt"
  #       - "prompts/json/dqi-theory.txt"
  #       - "prompts/json/json-schema.txt"
  #       - "prompts/json/example1-json.txt"
  #       - "prompts/json/example2-json.txt"
  #       - "prompts/json/closing-instructions.txt"
  #     include_previous_speeches: True
  #     k: 4
  #     n: 5000
  #     m: 1000
  #     p: 10000
  #     n_icl_examples: 0
  #   model_params:
  #     model: "deepseek-coder"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     preload_str: "Here is your requested JSON:\n{"
  #     prepend_str: "{"

  # - name: "gpt-4o-short"
  #   provider: "openai"
  #   api_key: "${OPENAI_API_KEY}"
  #   base_url: "https://api.openai.com/v1"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 3000
  #     include_previous_speeches: False
  #     n_icl_examples: 0
  #     elements:
  #       - "prompts/json/opening-instructions.txt"
  #       - "prompts/json/dqi-categories.txt"
  #       - "prompts/json/json-schema.txt"
  #       - "prompts/json/example1-json.txt"
  #       - "prompts/json/closing-instructions.txt"
  #   model_params:
  #     model: "gpt-4o-2024-05-13"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     preload_str: "Here is your requested JSON:\n{"
  #     prepend_str: "{"

  # - name: "gpt-4o-long"
  #   provider: "openai"
  #   api_key: "${OPENAI_API_KEY}"
  #   base_url: "https://api.openai.com/v1"
  #   prompt:
  #     elements:
  #       - "prompts/json/opening-instructions.txt"
  #       - "prompts/json/dqi-categories.txt"
  #       - "prompts/json/dqi-theory.txt"
  #       - "prompts/json/json-schema.txt"
  #       - "prompts/json/example1-json.txt"
  #       - "prompts/json/example2-json.txt"
  #       - "prompts/json/closing-instructions.txt"
  #     include_previous_speeches: True
  #     n_icl_examples: 0
  #     k: 4
  #     n: 5000
  #     m: 1000
  #     p: 10000
  #   model_params:
  #     model: "gpt-4o-2024-05-13"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     preload_str: "Here is your requested JSON:\n{"
  #     prepend_str: "{"

  # - name: "replicate-llama3-short"
  #   provider: "replicate"
  #   api_key: "${REPLICATE_API_KEY}"
  #   base_url: "https://api.replicate.com/v1"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 3000
  #     include_previous_speeches: False
  #     n_icl_examples: 0
  #     elements:
  #       - "prompts/json/opening-instructions.txt"
  #       - "prompts/json/dqi-categories.txt"
  #       - "prompts/json/json-schema.txt"
  #       - "prompts/json/example1-json.txt"
  #       - "prompts/json/closing-instructions.txt"
  #   model_params:
  #     model: "meta/meta-llama-3-70b-instruct"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     preload_str: "Here is your requested JSON:\n{"
  #     prepend_str: "{"

  # - name: "gpt-3.5-turbo-short"
  #   provider: "openai"
  #   api_key: "${OPENAI_API_KEY}"
  #   base_url: "https://api.openai.com/v1"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 3000
  #     include_previous_speeches: False
  #     n_icl_examples: 0
  #     elements:
  #       - "prompts/json/opening-instructions.txt"
  #       - "prompts/json/dqi-categories.txt"
  #       - "prompts/json/json-schema.txt"
  #       - "prompts/json/example1-json.txt"
  #       - "prompts/json/closing-instructions.txt"
  #   model_params:
  #     model: "gpt-3.5-turbo-0125"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     preload_str: "Here is your requested JSON:\n{"
  #     prepend_str: "{"

# API call settings
api_settings:
  rate_limit: 10  # Requests per minute
  concurrency: 10  # Maximum number of concurrent requests
  retry:
    max_attempts: 3
    initial_delay: 1  # seconds
    backoff_factor: 2
    max_delay: 60  # seconds

# Validation settings
validation:
  input:
    max_prompt_length: 4096  # Maximum allowed length for prompts
  output:
    required_fields: ["completion", "usage"]  # Fields that must be present in API response

logging:
  file: "api_interactions.log"
  directory: "logs"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  date_format: "%Y-%m-%d %H:%M:%S"
  max_file_size: 10485760  # 10 MB
  backup_count: 5

# Model metadata
model_metadata:
  - model: "claude-3-haiku-20240307"
    cost_per_million_input_tokens: 0.25
    cost_per_million_output_tokens: 1.25
    input_token_limit: 200000
    output_token_limit: 4000
  - model: "claude-3-5-sonnet-20240620"
    cost_per_million_input_tokens: 3.0
    cost_per_million_output_tokens: 15.0
    prompt_token_limit: 200000
    output_token_limit: 4000
  - model: "deepseek-chat"
    cost_per_million_input_tokens: 0.14
    cost_per_million_output_tokens: 0.28
    prompt_token_limit: 128000
    output_token_limit: 4000
  - model: "deepseek-coder"
    cost_per_million_input_tokens: 0.14
    cost_per_million_output_tokens: 0.28
    prompt_token_limit: 128000
    output_token_limit: 4000
  - model: "gpt-4o-2024-05-13"
    cost_per_million_input_tokens: 5.0
    cost_per_million_output_tokens: 15.0
    prompt_token_limit: 128000
    output_token_limit: 4000
  - model: "gpt-3.5-turbo-0125"
    cost_per_million_input_tokens: 0.5
    cost_per_million_output_tokens: 1.5
    prompt_token_limit: 16000
    output_token_limit: 4000
  - model: "llama3-8b-8192"
    cost_per_million_input_tokens: 0
    cost_per_million_output_tokens: 0
    prompt_token_limit: 8192
    output_token_limit: 4000
  - model: "llama3-70b-8192"
    cost_per_million_input_tokens: 0
    cost_per_million_output_tokens: 0
    prompt_token_limit: 8192
    output_token_limit: 4000
  - model: "gemma2-9b-it"
    cost_per_million_input_tokens: 0
    cost_per_million_output_tokens: 0
    prompt_token_limit: 8192
    output_token_limit: 4000
  - model: "mixtral-8x7b-32768"
    cost_per_million_input_tokens: 0
    cost_per_million_output_tokens: 0
    prompt_token_limit: 32768
    output_token_limit: 4000
  - model: "meta/meta-llama-3-70b-instruct"
    cost_per_million_input_tokens: 0.65
    cost_per_million_output_tokens: 2.75
    prompt_token_limit: 8000
    output_token_limit: 2048
