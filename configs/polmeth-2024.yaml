# Global settings
global:
  name: polmeth-2024
  test: false # If true, run with a small test dataset
  test_size: 1
  dry_run: false # If true, simulate API calls without actually making them
  log_level: "DEBUG" # Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  random_seed: 1990

# Data settings
data:
  raw_debates_dir: "data/raw/hein-daily"
  input_file: "data/raw/USfinal-clean.csv"
  merged_data_file: "data/polmeth-2024/clean/merged_us_data_subset.csv"
  validation_data: "data/polmeth-2024/clean/test_data.csv"
  api_response_directory: "data/polmeth-2024/api_responses"
  results_directory: "results/polmeth-2024/"
  prompts_directory: "data/polmeth-2024/prompts"
  clean_data_directory: "data/polmeth-2024/clean"
  train_data_file: "data/polmeth-2024/clean/train_data.csv"
  test_data_file: "data/polmeth-2024/clean/test_data.csv"
  merged_validation_results_train: "data/polmeth-2024/prompts/master_validation_results_train.csv"
  merged_validation_results_test: "data/polmeth-2024/prompts/master_validation_results_test.csv"
  example_set_directory: "data/polmeth-2024/icl_examples"
  test_size: 0.2  # Proportion of data to use for testing
  include_congresses:

# ICL settings
icl:
  use_in_depth_justifications: false  # Set to true for detailed justifications, false for minimalist approach
  justification_model:
    provider: "openai"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    model_params:
      model: "gpt-4"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: "Here is your justification:\n"
      prepend_str: ""

# JSON schema and healing settings
json_schema:
  path: "schemas/dqi_schema_simple.json"  # Path to your JSON schema file
  enabled: false
  healing:
    max_attempts: 3  # Maximum number of healing attempts per response
    healer_model:
      provider: "deepseek"
      api_key: "${DEEPSEEK_API_KEY}"
      base_url: "https://api.deepseek.com"
      model_params:
        model: "deepseek-coder"
        max_tokens: 4000
        temperature: 0.7
        top_p: 1
        preload_str: "Here is your requested JSON:\n{"
        prepend_str: "{"

analysis:
  confidence_level: 0.95
  significance_level: 0.05
  metrics_to_analyze:
    - mse
    - mae
    - correlation
    - accuracy
    - f1
    - oci
  aggregate_metrics:
    - avg_mse
    - avg_mae
    - avg_correlation
    - avg_accuracy
    - avg_f1
    - avg_oci
    - micro_f1
    - macro_f1
  tests:
    perform_t_tests: true
    perform_tukey_hsd: true
  export_confusion_matrices: true
  simplify_dimensions: true
  visualizations:
    generate_heatmaps: false
    generate_bar_plots: false
    generate_radar_charts: false
    generate_cost_comparison: true
    generate_performance_vs_cost: true
    generate_dimension_heatmap: false
    generate_significance_heatmap: false
    generate_performance_vs_icl: true  # New option for the ICL performance plot
    generate_initialization_comparison: false # Added to match the script
    generate_factor_analysis_plot: false
    generage_qq_plots: false
  initialization_analysis:
    generate_boxplots: true  # Added to match the script
  export:
    json: true
  dimension_level_analysis: true

# - `k`: This parameter determines how many previous speeches to include in the prompt. It specifies the number of previous speeches to look back from the current speech number.
# - `n`: This parameter limits the total length of the concatenated previous speeches text. It ensures that the final text does not exceed a certain length.
# - `m`: This parameter limits the length of each individual previous speech text. It ensures that each previous speech included in the final text does not exceed a certain length.
# - `p`: This parameter limits the length of the current speech text. It ensures that the text of the current speech does not exceed a certain length.
# Model configurations
models:
  - name: "deepseek-coder-icl-0"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 0
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-icl-5"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 5
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-icl-15"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 15
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-icl-25"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 25
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-icl-50"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 50
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-icl-100"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 100
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-bcot-icl-0"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 0
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
      closing_instructions: "prompts/json_v2/bayesian-cot-closing-instructions.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-bcot-icl-5"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 5
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
      closing_instructions: "prompts/json_v2/bayesian-cot-closing-instructions.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-bcot-icl-25"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 25
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
      closing_instructions: "prompts/json_v2/bayesian-cot-closing-instructions.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  - name: "deepseek-coder-bcot-icl-50"
    provider: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 50
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
      closing_instructions: "prompts/json_v2/bayesian-cot-closing-instructions.txt"
    model_params:
      model: "deepseek-coder"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: True
      top_logprobs: 5
      preload_str: ""
      prepend_str: ""

  # - name: "deepseek-coder-bcot-icl-100"
  #   provider: "deepseek"
  #   api_key: "${DEEPSEEK_API_KEY}"
  #   base_url: "https://api.deepseek.com"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 3000
  #     include_previous_speeches: True
  #     n_icl_examples: 100
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/bayesian-cot-closing-instructions.txt"
  #   model_params:
  #     model: "deepseek-coder"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: True
  #     top_logprobs: 5
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "deepseek-coder-icl-150"
  #   provider: "deepseek"
  #   api_key: "${DEEPSEEK_API_KEY}"
  #   base_url: "https://api.deepseek.com"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 3000
  #     include_previous_speeches: True
  #     n_icl_examples: 150
  #     n_random_init: 5
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #   model_params:
  #     model: "deepseek-coder"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: True
  #     top_logprobs: 5
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "deepseek-coder-long-cot-icl-0"
  #   provider: "deepseek"
  #   api_key: "${DEEPSEEK_API_KEY}"
  #   base_url: "https://api.deepseek.com"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 0
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/cot-closing-instructions.txt"
  #   model_params:
  #     model: "deepseek-coder"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: True
  #     top_logprobs: 5
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "deepseek-coder-long-cot-icl-5"
  #   provider: "deepseek"
  #   api_key: "${DEEPSEEK_API_KEY}"
  #   base_url: "https://api.deepseek.com"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 5
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/cot-closing-instructions.txt"
  #   model_params:
  #     model: "deepseek-coder"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: True
  #     top_logprobs: 5
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "deepseek-coder-long-cot-icl-15"
  #   provider: "deepseek"
  #   api_key: "${DEEPSEEK_API_KEY}"
  #   base_url: "https://api.deepseek.com"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 15
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/cot-closing-instructions.txt"
  #   model_params:
  #     model: "deepseek-coder"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: True
  #     top_logprobs: 15
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "deepseek-coder-long-cot-icl-50"
  #   provider: "deepseek"
  #   api_key: "${DEEPSEEK_API_KEY}"
  #   base_url: "https://api.deepseek.com"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 50
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/cot-closing-instructions.txt"
  #   model_params:
  #     model: "deepseek-coder"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: True
  #     top_logprobs: 5
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "deepseek-coder-long-cot-icl-100"
  #   provider: "deepseek"
  #   api_key: "${DEEPSEEK_API_KEY}"
  #   base_url: "https://api.deepseek.com"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 100
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/cot-closing-instructions.txt"
  #   model_params:
  #     model: "deepseek-coder"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: True
  #     top_logprobs: 5
  #     preload_str: ""
  #     prepend_str: ""


  - name: "claude-3-haiku-icl-0"
    provider: "anthropic"
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 0
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "claude-3-haiku-20240307"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  - name: "claude-3-haiku-icl-5"
    provider: "anthropic"
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 5
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "claude-3-haiku-20240307"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  - name: "claude-3-haiku-icl-25"
    provider: "anthropic"
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 25
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "claude-3-haiku-20240307"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""
      
  - name: "claude-3-haiku-icl-50"
    provider: "anthropic"
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 50
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "claude-3-haiku-20240307"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  - name: "claude-3-haiku-icl-100"
    provider: "anthropic"
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 100
      n_random_init: 5
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "claude-3-haiku-20240307"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  # - name: "claude-3-haiku-icl-150"
  #   provider: "anthropic"
  #   api_key: "${ANTHROPIC_API_KEY}"
  #   base_url: "https://api.anthropic.com"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 3000
  #     include_previous_speeches: True
  #     n_icl_examples: 100
  #     n_random_init: 5
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #   model_params:
  #     model: "claude-3-haiku-20240307"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "claude-3-5-sonnet-icl-25"
  #   provider: "anthropic"
  #   api_key: "${ANTHROPIC_API_KEY}"
  #   base_url: "https://api.anthropic.com"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 3000
  #     include_previous_speeches: True
  #     n_icl_examples: 25
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #   model_params:
  #     model: "claude-3-5-sonnet-20240620"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     preload_str: ""
  #     prepend_str: ""

  - name: "gpt-4o-icl-0"
    provider: "openai"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 0
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "gpt-4o-2024-05-13"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  - name: "gpt-4o-icl-5"
    provider: "openai"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 5
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "gpt-4o-2024-05-13"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  - name: "gpt-4o-icl-25"
    provider: "openai"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 25
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "gpt-4o-2024-05-13"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  - name: "gpt-4o-icl-50"
    provider: "openai"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 50
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "gpt-4o-2024-05-13"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  - name: "gpt-4o-icl-100"
    provider: "openai"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 100
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "gpt-4o-2024-05-13"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  - name: "meta-3-70b-instruct-icl-0"
    provider: "replicate"
    api_key: "${REPLICATE_API_KEY}"
    base_url: "https://api.replicate.com/v1"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 0
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "meta/meta-llama-3-70b-instruct"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  - name: "meta-3-70b-instruct-icl-5"
    provider: "replicate"
    api_key: "${REPLICATE_API_KEY}"
    base_url: "https://api.replicate.com/v1"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 5
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "meta/meta-llama-3-70b-instruct"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  - name: "meta-3-8b-instruct-icl-0"
    provider: "replicate"
    api_key: "${REPLICATE_API_KEY}"
    base_url: "https://api.replicate.com/v1"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 0
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "meta/meta-llama-3-8b-instruct"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  - name: "meta-3-8b-instruct-icl-5"
    provider: "replicate"
    api_key: "${REPLICATE_API_KEY}"
    base_url: "https://api.replicate.com/v1"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 3000
      include_previous_speeches: True
      n_icl_examples: 5
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "meta/meta-llama-3-8b-instruct"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      preload_str: ""
      prepend_str: ""

  # - name: "together-wizardlm2moe-long-cot-icl-10"
  #   provider: "together"
  #   api_key: "${TOGETHER_API_KEY}"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 10
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/cot-closing-instructions.txt"
  #   model_params:
  #     model: "microsoft/WizardLM-2-8x22b"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: 0
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "together-wizardlm2moe-long-cot-icl-15"
  #   provider: "together"
  #   api_key: "${TOGETHER_API_KEY}"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 15
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/cot-closing-instructions.txt"
  #   model_params:
  #     model: "microsoft/WizardLM-2-8x22b"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: 0
  #     preload_str: ""
  #     prepend_str: ""

  - name: "together-qwen2-72b-icl-0"
    provider: "together"
    api_key: "${TOGETHER_API_KEY}"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 10000
      include_previous_speeches: True
      n_icl_examples: 0
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "Qwen/Qwen2-72b-Instruct"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: 0
      preload_str: ""
      prepend_str: ""

  - name: "together-qwen2-72b-icl-5"
    provider: "together"
    api_key: "${TOGETHER_API_KEY}"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 10000
      include_previous_speeches: True
      n_icl_examples: 5
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "Qwen/Qwen2-72b-Instruct"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: 0
      preload_str: ""
      prepend_str: ""

  - name: "together-qwen2-72b-icl-15"
    provider: "together"
    api_key: "${TOGETHER_API_KEY}"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 10000
      include_previous_speeches: True
      n_icl_examples: 15
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "Qwen/Qwen2-72b-Instruct"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: 0
      preload_str: ""
      prepend_str: ""

  - name: "together-qwen2-72b-icl-25"
    provider: "together"
    api_key: "${TOGETHER_API_KEY}"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 10000
      include_previous_speeches: True
      n_icl_examples: 25
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "Qwen/Qwen2-72b-Instruct"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: 0
      preload_str: ""
      prepend_str: ""

  # - name: "together-qwen2-72b-long-cot-icl-15"
  #   provider: "together"
  #   api_key: "${TOGETHER_API_KEY}"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 15
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/cot-closing-instructions.txt"
  #   model_params:
  #     model: "Qwen/Qwen2-72b-Instruct"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: 0
  #     preload_str: ""
  #     prepend_str: ""

  - name: "together-wizard2-icl-0"
    provider: "together"
    api_key: "${TOGETHER_API_KEY}"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 10000
      include_previous_speeches: True
      n_icl_examples: 0
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "microsoft/WizardLM-2-8x22b"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: 0
      preload_str: ""
      prepend_str: ""

  - name: "together-wizard2-icl-5"
    provider: "together"
    api_key: "${TOGETHER_API_KEY}"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 10000
      include_previous_speeches: True
      n_icl_examples: 5
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "microsoft/WizardLM-2-8x22b"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: 0
      preload_str: ""
      prepend_str: ""

  - name: "together-wizard2-icl-15"
    provider: "together"
    api_key: "${TOGETHER_API_KEY}"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 10000
      include_previous_speeches: True
      n_icl_examples: 15
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "microsoft/WizardLM-2-8x22b"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: 0
      preload_str: ""
      prepend_str: ""

  - name: "together-wizard2-icl-25"
    provider: "together"
    api_key: "${TOGETHER_API_KEY}"
    prompt:
      k: 4
      n: 2000
      m: 500
      p: 10000
      include_previous_speeches: True
      n_icl_examples: 25
      n_random_init: 1
      elements:
        - "prompts/json_v2/opening-instructions.txt"
        - "prompts/json_v2/dqi-theory.txt"
        - "prompts/json_v2/dqi-categories.txt"
        - "prompts/json_v2/json-schema.txt"
    model_params:
      model: "microsoft/WizardLM-2-8x22b"
      max_tokens: 4000
      temperature: 0.7
      top_p: 1
      logprobs: 0
      preload_str: ""
      prepend_str: ""

  # - name: "together-wizard2-bcot-icl-0"
  #   provider: "together"
  #   api_key: "${TOGETHER_API_KEY}"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 0
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/bayesian-cot-closing-instructions.txt"
  #   model_params:
  #     model: "microsoft/WizardLM-2-8x22b"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: 0
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "together-wizard2-bcot-icl-5"
  #   provider: "together"
  #   api_key: "${TOGETHER_API_KEY}"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 5
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/bayesian-cot-closing-instructions.txt"
  #   model_params:
  #     model: "microsoft/WizardLM-2-8x22b"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: 0
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "together-wizard2-bcot-icl-15"
  #   provider: "together"
  #   api_key: "${TOGETHER_API_KEY}"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 15
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/bayesian-cot-closing-instructions.txt"
  #   model_params:
  #     model: "microsoft/WizardLM-2-8x22b"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: 0
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "together-wizard2-bcot-icl-25"
  #   provider: "together"
  #   api_key: "${TOGETHER_API_KEY}"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 25
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #     closing_instructions: "prompts/json_v2/bayesian-cot-closing-instructions.txt"
  #   model_params:
  #     model: "microsoft/WizardLM-2-8x22b"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: 0
  #     preload_str: ""
  #     prepend_str: ""

  # - name: "together-gemma2-27b-icl-0"
  #   provider: "together"
  #   api_key: "${TOGETHER_API_KEY}"
  #   prompt:
  #     k: 4
  #     n: 2000
  #     m: 500
  #     p: 10000
  #     include_previous_speeches: True
  #     n_icl_examples: 0
  #     n_random_init: 1
  #     elements:
  #       - "prompts/json_v2/opening-instructions.txt"
  #       - "prompts/json_v2/dqi-theory.txt"
  #       - "prompts/json_v2/dqi-categories.txt"
  #       - "prompts/json_v2/json-schema.txt"
  #   model_params:
  #     model: "google/gemma-2-27b-it"
  #     max_tokens: 4000
  #     temperature: 0.7
  #     top_p: 1
  #     logprobs: 0
  #     preload_str: ""
  #     prepend_str: ""

# API call settings
api_settings:
  rate_limit: 50 # Requests per minute
  concurrency: 50 # Maximum number of concurrent requests
  retry:
    max_attempts: 3
    initial_delay: 1  # seconds
    backoff_factor: 2
    max_delay: 60  # seconds

# Validation settings
validation:
  input:
    max_prompt_length: 4096  # Maximum allowed length for prompts
  output:
    required_fields: ["completion", "usage"]  # Fields that must be present in API response

logging:
  file: "api_interactions.log"
  directory: "logs"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  date_format: "%Y-%m-%d %H:%M:%S"
  max_file_size: 10485760  # 10 MB
  backup_count: 5

# Model metadata
model_metadata:
  - model: "claude-3-haiku-20240307"
    cost_per_million_input_tokens: 0.25
    cost_per_million_output_tokens: 1.25
    input_token_limit: 200000
    output_token_limit: 4000
  - model: "claude-3-5-sonnet-20240620"
    cost_per_million_input_tokens: 3.0
    cost_per_million_output_tokens: 15.0
    prompt_token_limit: 200000
    output_token_limit: 4000
  - model: "deepseek-chat"
    cost_per_million_input_tokens: 0.14
    cost_per_million_output_tokens: 0.28
    prompt_token_limit: 128000
    output_token_limit: 4000
  - model: "deepseek-coder"
    cost_per_million_input_tokens: 0.14
    cost_per_million_output_tokens: 0.28
    prompt_token_limit: 128000
    output_token_limit: 4000
  - model: "gpt-4o-2024-05-13"
    cost_per_million_input_tokens: 5.0
    cost_per_million_output_tokens: 15.0
    prompt_token_limit: 128000
    output_token_limit: 4000
  - model: "gpt-3.5-turbo-0125"
    cost_per_million_input_tokens: 0.5
    cost_per_million_output_tokens: 1.5
    prompt_token_limit: 16000
    output_token_limit: 4000
  - model: "llama3-8b-8192"
    cost_per_million_input_tokens: 0
    cost_per_million_output_tokens: 0
    prompt_token_limit: 8192
    output_token_limit: 4000
  - model: "llama3-70b-8192"
    cost_per_million_input_tokens: 0
    cost_per_million_output_tokens: 0
    prompt_token_limit: 8192
    output_token_limit: 4000
  - model: "gemma2-9b-it"
    cost_per_million_input_tokens: 0
    cost_per_million_output_tokens: 0
    prompt_token_limit: 8192
    output_token_limit: 4000
  - model: "mixtral-8x7b-32768"
    cost_per_million_input_tokens: 0
    cost_per_million_output_tokens: 0
    prompt_token_limit: 32768
    output_token_limit: 4000
  - model: "meta/meta-llama-3-70b-instruct"
    cost_per_million_input_tokens: 0.65
    cost_per_million_output_tokens: 2.75
    prompt_token_limit: 8000
    output_token_limit: 2048
